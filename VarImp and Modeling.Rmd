---
title: "BDB Modelling"
author: "Trevor Hughes"
date: "11/28/2021"
output: html_document
---

```{r}
library(dplyr)
library(smbinning)
library(corpcor)
library(car)
library(rpart)
library(rpart.plot)
library(ggplot2)
library(xgboost)
library(caret)
library(Ckmeans.1d.dp)
```


#read in data
```{r}
train = read.csv("C:\\Users\\thughes\\Documents\\model train set.csv")
val = read.csv("C:\\Users\\thughes\\Documents\\model val set.csv")
```

```{r}
#Create decision tree of yardlineTotal to see where significant splits are
fit = rpart(kickReturnYardage ~ yardlineTotal , data = train, cp = .001, minbucket = 50)
rpart.plot(fit)

```

```{r}
#Change all variables with less than 16 unique values to factor
for (i in 1:length(train)) {
  if (length(unique(train[,i])) < 16) {
    train[,i] = as.factor(train[,i])
  }
}
  
train = train %>% filter(is.na(train$kickReturnYardage) == 0)

for (i in 1:length(val)) {
  if (length(unique(val[,i])) < 16) {
    val[,i] = as.factor(val[,i])
  }
}
  
val = val %>% filter(is.na(val$kickReturnYardage) == 0)

for (i in 1:length(val)) {
  if (length(unique(val[,i])) < 16) {
    val[,i] = as.factor(val[,i])
  }
}
  
val = val %>% filter(is.na(val$kickReturnYardage) == 0)


#colnames(train[38])
#vif( lm(kickReturnYardage ~ . , data = train[,c(4:51, 55:60)]))

# Distances for players 3 through 8 have vif's above 10
#intended and actual return directions of course have high vif as well
#return_x and yardlineTotal have high vifs
```

#Getting variable importance from decision tree
```{r}
fit = rpart(kickReturnYardage ~ . , data = train[,c(4:40, 52:71)])
rpart.plot(fit)

varimp.data = data.frame(fit$variable.importance)
varimp.data$names = as.character(rownames(varimp.data))
varimp.data
#ggplot(data = varimp.data, aes(x = names, y = fit.variable.importance)) + geom_bar(stat = "identity")
```

```{r}
#Not including zones, return direction actual, quarter
train_x = model.matrix(kickReturnYardage ~ ., data = train[,c(4:6, 8:40,52:64,68:71 )])[,-1]
train_y = train$kickReturnYardage

set.seed(123)
xgb.punt = xgboost(data = train_x, label = train_y, subsample = .5, nrounds = 100)

set.seed(123)
xgbcv.punt = xgb.cv(data = train_x, label = train_y, subsample = .5, nrounds = 100, nfold = 10, metrics = 'mae')
#Optimal nrounds is 4
```

```{r}
tune_grid = expand.grid(
  nrounds = 4,
  eta = c(.1, .15, .2, .25, .3),
  max_depth = c(1:10),
  gamma = c(0),
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = c(.25, .5, .75, 1)
)

set.seed(123)
xgb.punt.caret = train(x = train_x, y = train_y, method = "xgbTree", tuneGrid = tune_grid, trControl = trainControl(method = 'cv', number = 10), metric = 'MAE')

plot(xgb.punt.caret)
xgb.punt.caret$bestTune
```

```{r}
set.seed(123)
xgb.punt.final = xgboost(data = train_x, label = train_y, subsample = .75, nrounds = 4, eta = .3, max_depth = 2, prediction = T, eval_metric = 'mae')

xgb.importance(feature_names = colnames(train_x), model = xgb.punt.final)
xgb.ggplot.importance(xgb.importance(feature_names = colnames(train_x), model = xgb.punt.final))

#MAE of 5.9
#MAE of 5.7 with sides instead of zones
#MAE of 5.9 without
```



#Create different data sets depending on zone groups
```{r}
train.a1 = train %>% filter(return_zone %in% c(1,2,3))
train.a2 = train %>% filter(return_zone %in% c(4,5,6))
train.a3 = train %>% filter(return_zone %in% c(7,8,9))

val.a1 = val %>% filter(return_zone %in% c(1,2,3))
val.a2 = val %>% filter(return_zone %in% c(4,5,6))
val.a3 = val %>% filter(return_zone %in% c(7,8,9))
```


```{r}
varlist = list(c(4:6, 8:40,63, 65,67:71 ),c(4:6, 8:40,63, 64,67:71 ),
               c(4:6, 8:40,52:71 ), c(4:6, 8:40,52:65, 67:71 ), c(4:6, 8:40,63:71 ),
               c(4:40,63:71 ), c(4:6, 8:40,63,64,66:71 ), c(5:6, 8:40,63:71 ), 
               c(4:6, 8:40,63:67, 69:71 ), c(4:6, 8:40,63:68, 70, 71 ), 
               c(4:6, 8:40,63:66, 68:71 ), c(4:6, 8:40,63,64,68:71 ))


num = 0
#for (vars in varlist){
#Placeholder loop if not looping through different groups of variables
for (j in 1) {
  num = num + 1
  for (i in 1:3){
    print(paste("Area ", i))
    zones = ifelse(i == 1, list(1:3), ifelse(i == 2, list(4:6), list(7:9)))
    train.a = train %>% filter(return_zone %in% zones[[1]])
  
    #No sides or zones of defenders included
    train_x = model.matrix(kickReturnYardage ~ ., data = train.a[,c(4:6, 8:40,63:68, 70, 71 )])[,-1]
    train_y = train.a$kickReturnYardage
  
    tune_grid = expand.grid(
    nrounds = 4,
    eta = c(.1, .15, .2, .25, .3),
    max_depth = c(1:10),
    gamma = c(0),
    colsample_bytree = 1,
    min_child_weight = 1,
    subsample = c(.25, .5, .75, 1)
    )

    set.seed(123)
    xgb.punt.caret = train(x = train_x, y = train_y, method = "xgbTree", tuneGrid = tune_grid, trControl = trainControl(method = 'cv', number = 10), metric = 'MAE')

    set.seed(123)
    assign(paste0("xgb.punt.a", i), xgboost(data = train_x, label = train_y, 
                           subsample = xgb.punt.caret$bestTune$subsample, nrounds = 4, 
                           eta = xgb.punt.caret$bestTune$eta,
                           max_depth = xgb.punt.caret$bestTune$max_depth, 
                           prediction = T, eval_metric = 'mae'))

    set.seed(123)
    xgb.punt.a = xgboost(data = train_x, label = train_y, 
                           subsample = xgb.punt.caret$bestTune$subsample, nrounds = 4, 
                           eta = xgb.punt.caret$bestTune$eta,
                           max_depth = xgb.punt.caret$bestTune$max_depth, 
                           prediction = T, eval_metric = 'mae')


    #x = xgb.importance(feature_names = colnames(train_x), model = xgb.punt.final)
    #print(x)
    
    g = xgb.ggplot.importance(xgb.importance(feature_names = colnames(train_x), model = xgb.punt.a)) + ggtitle(paste("Feature Importance Area", i))
    print(g)
  }

  averaged_mae = (min(xgb.punt.a1$evaluation_log$train_mae)*nrow(train.a1) +
      min(xgb.punt.a2$evaluation_log$train_mae)*nrow(train.a2) +
      min(xgb.punt.a3$evaluation_log$train_mae)*nrow(train.a3))/nrow(train)
  
  print(num)
  print(vars)
  print(averaged_mae)
}
for (i in 1:3){
    print(paste("Area ", i))
    
    zones = ifelse(i == 1, list(1:3), ifelse(i == 2, list(4:6), list(7:9)))
    train.a = train %>% filter(return_zone %in% zones[[1]])
    print(zones)
    print(nrow(train.a))
}

#1 = 5.180
#2 = 5.374
#3 = 5.282
#4 = 5.285
#5 = 5.208
#6 = 5.208
#7 = 5.369
#8 = 5.210
#9 = 5.206
#10 = 5.084
#11 = 5.212
#12 = 5.367

#Weighted MAE of 5.084 best for variables: c(4:6, 8:40,63:68, 70, 71 )
#Weighted MAE of 5.180 best for variables: c(4:6, 8:40,63, 65,67:71 )
### If we can't include actual return direction
#Weighted MAE of 5.367 best for variables: c(4:6, 8:40,63,64,68:71 )

```

#Run area xgboost on validation
```{r}
area_xgbs = list(xgb.punt.a1, xgb.punt.a2, xgb.punt.a3)

for (i in 1:3){
  zones = ifelse(i == 1, list(1:3), ifelse(i == 2, list(4:6), list(7:9)))
  val.a = val %>% filter(return_zone %in% zones[[1]])
  
  val_x = model.matrix(kickReturnYardage ~ ., data = val.a[,c(4:6, 8:40,63:68, 70, 71 )])[,-1]
  val_y = val.a$kickReturnYardage
  
  val.a$xgb_area_pred = predict(area_xgbs[i], type = "response", val_x)[[1]]
  
  assign(paste0("val.mae.a", i), MAE(val.a$xgb_area_pred, val.a$kickReturnYardage))
}


(val.mae.a1*nrow(val.a1) + val.mae.a2*nrow(val.a2) + val.mae.a3*nrow(val.a3))/nrow(val)

#Weighted Val MAE of 5.199 for c(4:6, 8:40,63:68, 70, 71 )

```


############################## XGBoost by Zone ###########################
#Create different data sets depending on zone
```{r}
train.z1 = train %>% filter(return_zone == 1)
train.z2 = train %>% filter(return_zone == 2)
train.z3 = train %>% filter(return_zone == 3)
train.z4 = train %>% filter(return_zone == 4)
train.z5 = train %>% filter(return_zone == 5)
train.z6 = train %>% filter(return_zone == 6)
train.z7 = train %>% filter(return_zone == 7)
train.z8 = train %>% filter(return_zone == 8)
train.z9 = train %>% filter(return_zone == 9)

val.z1 = val %>% filter(return_zone == 1)
val.z2 = val %>% filter(return_zone == 2)
val.z3 = val %>% filter(return_zone == 3)
val.z4 = val %>% filter(return_zone == 4)
val.z5 = val %>% filter(return_zone == 5)
val.z6 = val %>% filter(return_zone == 6)
val.z7 = val %>% filter(return_zone == 7)
val.z8 = val %>% filter(return_zone == 8)
val.z9 = val %>% filter(return_zone == 9)

```


```{r}
varlist = list(c(4:6, 8:40,52:66,68:71 ), c(4:6, 8:40,63:65, 68:71 ),
               c(4:6, 8:40,63, 65,68:71 ),c(4:6, 8:40,63, 64,68:71 ),
               c(4:6, 8:40,52:63, 65,68:71 ),c(4:6, 8:40,52:64,68:71 ),
               c(4:6, 8:40,63, 65, 66,68:71 ),c(4:6, 8:40,63, 64, 66,68:71 ),
               c(4:6, 8:40,52:63, 65, 66,68:71 ),c(4:6, 8:40,52:64, 66,68:71 ))


num = 0
#for (vars in varlist){
#Placeholder loop if not looping through different groups of variables
for (j in 1) {
  num = num + 1
  for (i in 1:9){
    print(paste("Zone ", i))
    train.z = train %>% filter(return_zone == i)
    train_x = model.matrix(kickReturnYardage ~ ., data = train.z[,c(4:6, 8:40,52:64,68:71 )])[,-1]
    train_y = train.z$kickReturnYardage
  
    tune_grid = expand.grid(
    nrounds = 4,
    eta = c(.1, .15, .2, .25, .3),
    max_depth = c(1:10),
    gamma = c(0),
    colsample_bytree = 1,
    min_child_weight = 1,
    subsample = c(.25, .5, .75, 1)
    )

    set.seed(123)
    xgb.punt.caret = train(x = train_x, y = train_y, method = "xgbTree", tuneGrid = tune_grid, trControl = trainControl(method = 'cv', number = 10), metric = 'MAE')

    set.seed(123)
    assign(paste0("xgb.punt.z", i), xgboost(data = train_x, label = train_y, 
                           subsample = xgb.punt.caret$bestTune$subsample, nrounds = 4, 
                           eta = xgb.punt.caret$bestTune$eta,
                           max_depth = xgb.punt.caret$bestTune$max_depth, 
                           prediction = T, eval_metric = 'mae'))

    set.seed(123)
    xgb.punt.z = xgboost(data = train_x, label = train_y, 
                           subsample = xgb.punt.caret$bestTune$subsample, nrounds = 4, 
                           eta = xgb.punt.caret$bestTune$eta,
                           max_depth = xgb.punt.caret$bestTune$max_depth, 
                           prediction = T, eval_metric = 'mae')


    #x = xgb.importance(feature_names = colnames(train_x), model = xgb.punt.final)
    #print(x)
    #g = xgb.ggplot.importance(xgb.importance(feature_names = colnames(train_x), model = xgb.punt.z)) + ggtitle(paste("Feature Importance Zone", i))
    #print(g)
  }

  averaged_mae = (min(xgb.punt.z1$evaluation_log$train_mae)*nrow(train.z1) + min(xgb.punt.z2$evaluation_log$train_mae)*nrow(train.z2) + min(xgb.punt.z3$evaluation_log$train_mae)*nrow(train.z3) + min(xgb.punt.z4$evaluation_log$train_mae)*nrow(train.z4) + min(xgb.punt.z5$evaluation_log$train_mae)*nrow(train.z5) + min(xgb.punt.z6$evaluation_log$train_mae)*nrow(train.z6) + min(xgb.punt.z7$evaluation_log$train_mae)*nrow(train.z7) + min(xgb.punt.z8$evaluation_log$train_mae)*nrow(train.z8) + min(xgb.punt.z9$evaluation_log$train_mae)*nrow(train.z9))/nrow(train)

  print(num)
  print(vars)
  print(averaged_mae)
}


#2 = 5.072
#3 = 5.197
#4 = 5.350
#5 = 5.368
#6 = 4.863
#7 = 5.245
#8 = 5.534
#9 = 5.363
#10 = 4.863
#xgb.punt.z6$feature_names

### Best overall
#Weighted MAE of 4.863 best for variables: c(4:6, 8:40,52:64,68:71 )
### Best that didn't include sides
#Weighted MAE of 5.072 best for variables: c(4:6, 8:40,63:65, 68:71 ) 
### If we can't include intended return direction
#Weighted MAE of 5.197 best for variables: c(4:6, 8:40,63, 65,68:71 )
### If we can't include actual return direction and the defensive player sides
#Weighted MAE of 5.350 best for variables: c(4:6, 8:40,63, 64,68:71 ) 
```




#Run zone xgboost on validation
```{r}
zone_xgbs = list(xgb.punt.z1, xgb.punt.z2, xgb.punt.z3, xgb.punt.z4, xgb.punt.z5, xgb.punt.z6, xgb.punt.z7, xgb.punt.z8, xgb.punt.z9)

for (i in 1:9){
  val.z = train %>% filter(return_zone == i)
  
  val_x = model.matrix(kickReturnYardage ~ ., data = val.z[,c(4:6, 8:40,52:64,68:71 )])[,-1]
  val_y = val.z$kickReturnYardage
  
  val.z$xgb_zone_pred = predict(zone_xgbs[i], type = "response", val_x)[[1]]
  
  assign(paste0("val.mae.z", i), MAE(val.z$xgb_zone_pred, val.z$kickReturnYardage))
}


(val.mae.z1*nrow(val.z1) + val.mae.z2*nrow(val.z2) + val.mae.z3*nrow(val.z3) +
  val.mae.z4*nrow(val.z4) + val.mae.z5*nrow(val.z5) + val.mae.z6*nrow(val.z6) +
  val.mae.z7*nrow(val.z7) + val.mae.z8*nrow(val.z8) + val.mae.z9*nrow(val.z9))/nrow(val)

#Weighted Val MAE of 4.772 for c(4:6, 8:40,52:64,68:71 )

```
