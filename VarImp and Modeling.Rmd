---
title: "BDB Modelling"
author: "Trevor Hughes"
date: "11/28/2021"
output: html_document
---

```{r}
library(dplyr)
library(smbinning)
library(corpcor)
library(car)
library(rpart)
library(rpart.plot)
library(ggplot2)
library(xgboost)
library(caret)
library(Ckmeans.1d.dp)
library(nnet)
library(NeuralNetTools)
library(Metrics)
```


#read in data
```{r}
model_set = read.csv("C:\\Users\\thughes\\Documents\\model set.csv")
train = read.csv("C:\\Users\\thughes\\Documents\\model train set.csv")
val = read.csv("C:\\Users\\thughes\\Documents\\model val set.csv")
full_set = read.csv("C:\\Users\\thughes\\Documents\\full set.csv")


```

```{r}
#Create decision tree of yardlineTotal to see where significant splits are
fit = rpart(kickReturnYardage ~ return_x , data = train, cp = .0025, minbucket = 50)
rpart.plot(fit)


```

```{r}
#Change all variables with less than 16 unique values to factor
for (i in 1:length(train)) {
  if (length(unique(train[,i])) < 16) {
    train[,i] = as.factor(train[,i])
  }
}

train = train %>% filter(is.na(train$kickReturnYardage) == 0)

for (i in 1:length(val)) {
  if (length(unique(val[,i])) < 16) {
    val[,i] = as.factor(val[,i])
  }
}
  
val = val %>% filter(is.na(val$kickReturnYardage) == 0)

for (i in 1:length(full_set)) {
  if (length(unique(full_set[,i])) < 16) {
    full_set[,i] = as.factor(full_set[,i])
  }
}
  
full_set = full_set %>% filter(is.na(full_set$kickReturnYardage) == 0)


#colnames(train[38])
#vif( lm(kickReturnYardage ~ . , data = train[,c(4:51, 55:60)]))

# Distances for players 3 through 8 have vif's above 10
#intended and actual return directions of course have high vif as well
#return_x and yardlineTotal have high vifs
```

#Getting variable importance from decision tree
```{r}
fit = rpart(kickReturnYardage ~ . , data = train[,c(4:40, 52:71)])
rpart.plot(fit)

varimp.data = data.frame(fit$variable.importance)
varimp.data$names = as.character(rownames(varimp.data))
varimp.data
#ggplot(data = varimp.data, aes(x = names, y = fit.variable.importance)) + geom_bar(stat = "identity")
```

```{r}
#Not including zones, return direction actual, quarter
train_x = model.matrix(kickReturnYardage ~ ., data = train[,c(4:6, 8:51,63:71 )])[,-1]
train_y = train$kickReturnYardage

set.seed(123)
xgb.punt = xgboost(data = train_x, label = train_y, subsample = .5, nrounds = 100)

set.seed(123)
xgbcv.punt = xgb.cv(data = train_x, label = train_y, subsample = .5, nrounds = 100, nfold = 10, metrics = 'mae')
#Optimal nrounds is 4
```

```{r}
tune_grid = expand.grid(
  nrounds = 4,
  eta = c(.1, .15, .2, .25, .3),
  max_depth = c(1:10),
  gamma = c(0),
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = c(.25, .5, .75, 1)
)

set.seed(123)
xgb.punt.caret = train(x = train_x, y = train_y, method = "xgbTree", tuneGrid = tune_grid, trControl = trainControl(method = 'cv', number = 10), metric = 'MAE')

plot(xgb.punt.caret)
xgb.punt.caret$bestTune
```

```{r}
set.seed(123)
xgb.punt.final = xgboost(data = train_x, label = train_y, subsample = .75, nrounds = 4, eta = .3, max_depth = 2, prediction = T, eval_metric = 'mae')

xgb.importance(feature_names = colnames(train_x), model = xgb.punt.final)
xgb.ggplot.importance(xgb.importance(feature_names = colnames(train_x), model = xgb.punt.final))

#MAE of 5.9
#MAE of 5.7 with sides instead of zones
#MAE of 5.9 without
```


```{r}
train.h1 = train %>% filter(return_x < 28)
train.h2 = train %>% filter(return_x >= 28)

levels(val$return_zone) = c(levels(val$return_zone), "9")
val.h1 = val %>% filter(return_x < 28)
val.h2 = val %>% filter(return_x >= 28)

```


```{r}
varlist = list(c(4:6, 8:40,63, 64,67:71 ),
               c(4:40,63,64,66:71 ),  c(5:6, 8:40,63,64,66:71 ), 
               c(4:6, 8:40,63,64,66:67, 69:71 ),  
               c(4:6, 8:40,63,64,66, 68:71 ),c(4,6, 8:40,63,64,66:71,74))

varlist2=  list(c(5, 8:40,63,64,66:70 ), c(5, 8:40,63,64,66:68,70,71 ))
                   
varlist3 = list(c(4:5, 8:40,63,64,66:71 ),c(4:6, 8:40,64,66:71 ),c(4:6, 8:40,63,64,67:71 ),c(4:6, 8:40,63,64,66,67,69:71 ),c(4:6, 8:40,63,64,66:70 ), c(4:6, 8:40,63,64,66:68,70,71 ), c(4:6, 8:40,63,64,66,68:71 ) )

num = 0
#for (vars in varlist2){
#Placeholder loop if not looping through different groups of variables
for (j in 1) {
  num = num + 1
  for (i in 1:2){
    print(paste("Half ", i))
    
    if (i == 1) {
      train.h = train %>% filter(return_x < 28)
    }
    else {
      train.h = train %>% filter(return_x >= 28)
    }
    #No sides or zones of defenders included
    train_x = model.matrix(kickReturnYardage ~ ., data = train.h[,c(5, 8:40,63,64,66:71 )])[,-1]
    train_y = train.h$kickReturnYardage
  
    tune_grid = expand.grid(
    nrounds = 4,
    eta = c(.1, .15, .2, .25, .3),
    max_depth = c(1:10),
    gamma = c(0),
    colsample_bytree = 1,
    min_child_weight = 1,
    subsample = c(.25, .5, .75, 1)
    )

    set.seed(123)
    xgb.punt.caret = train(x = train_x, y = train_y, method = "xgbTree", tuneGrid = tune_grid, trControl = trainControl(method = 'cv', number = 10), metric = 'MAE')

    set.seed(123)
    assign(paste0("xgb.punt.h", i), xgboost(data = train_x, label = train_y, 
                           subsample = xgb.punt.caret$bestTune$subsample, nrounds = 4, 
                           eta = xgb.punt.caret$bestTune$eta,
                           max_depth = xgb.punt.caret$bestTune$max_depth, 
                           prediction = T, eval_metric = 'mae'))

    set.seed(123)
    xgb.punt.h = xgboost(data = train_x, label = train_y, 
                           subsample = xgb.punt.caret$bestTune$subsample, nrounds = 4, 
                           eta = xgb.punt.caret$bestTune$eta,
                           max_depth = xgb.punt.caret$bestTune$max_depth, 
                           prediction = T, eval_metric = 'mae')


    #x = xgb.importance(feature_names = colnames(train_x), model = xgb.punt.final)
    #print(x)
    
    g = xgb.ggplot.importance(xgb.importance(feature_names = colnames(train_x), model = xgb.punt.h)) + ggtitle(paste("Feature Importance Half", i))
    print(g)
  }

  averaged_mae = (min(xgb.punt.h1$evaluation_log$train_mae)*nrow(train.h1) +
      min(xgb.punt.h2$evaluation_log$train_mae)*nrow(train.h2))/nrow(train)
  
  print(num)
  print(vars)
  print(averaged_mae)
}

#c(5:6, 8:40,63,64,66:71 ) MAE of 4.814
#Average MAE of c(4:6, 8:40,63,64,66:71 ) = 4.835 

#c(5, 8:40,63,64,66:71 ) MAE of 4.729

```

#Run halves xgboost on validation
```{r}
half_xgbs = list(xgb.punt.h1, xgb.punt.h2)
val.preds.h1 = data.frame()
val.preds.h2 = data.frame()

for (i in 1:2){
  if (i == 1) {
      val.h = val %>% filter(return_x < 28)
    }
    else {
      val.h = val %>% filter(return_x >= 28)
    }
  
  val_x = model.matrix(kickReturnYardage ~ ., data = val.h[,c(5, 8:40,63,64,66:71 )])[,-1]
  val_y = val.h$kickReturnYardage
  
  val.h$xgb_half_pred = predict(half_xgbs[i], type = "response", val_x)[[1]]
  
  assign(paste0("val.mae.h", i), MAE(val.h$xgb_half_pred, val.h$kickReturnYardage))
  
  #Storing predictions for ensemble model
  assign(paste0("val.preds.h",i), val.h)
}



(val.mae.h1*nrow(val.h1) + val.mae.h2*nrow(val.h2))/nrow(val)

#Weighted Val MAE of 5.190 for c(4:6, 8:40,63:71 )
#Weighted Val MAE of 5.187 for c(4:40,63:68,70,71 )


#Weighted Val MAE of 5.314 for c(5, 8:40,63,64,66:71 ) 

val.preds.xgb = rbind(val.preds.h1, val.preds.h2)
```



################################## Liam's NN Model ###################################

```{r}
#Getting variable importance from decision tree
fit = rpart(train$kickReturnYardage ~ . , data = train[,c(4:40,52:64,66:71)])
rpart.plot(fit)
varimp.data = data.frame(fit$variable.importance)
varimp.data$names = as.character(rownames(varimp.data))
varimp.data


train2 = train
val2 = val

# Scale standardize 
trainScale = preProcess(train2[4:71])
train2 = cbind( train2[1:3], predict(trainScale, train2[4:71]))
val2 = cbind( val2[1:3], predict(trainScale, val2[4:71]))



# Check columns data types
sapply(train2, class) # Dataset contains numeric, integer, and factor variables

# Using varimp.data output. Cutoff of 700 for fit.variable.importance
var.sel <- varimp.data$names

train2 <- train2 %>% 
  select(var.sel, kickReturnYardage, X, gameId, playId)

val2 <- val2 %>% 
  select(var.sel, X, kickReturnYardage, gameId, playId)

#Create Neural Network
set.seed(123)
nn.yard.model <- nnet(kickReturnYardage ~ . -gameId -playId -X, data = train2, size = 6, decay = .945, linout = TRUE )

nn.yard.val.predict <- predict(nn.yard.model, newdata = val2)

val.unscale.predictions = sd(train$kickReturnYardage, na.rm = TRUE) * nn.yard.val.predict + mean(train$kickReturnYardage, na.rm = TRUE)

# Calculate RMSE and MAE
val.rmse <- sqrt(mean((nn.yard.val.predict - val2$kickReturnYardage)^2)) # RMSE
val.mae <- mae(val2$kickReturnYardage, nn.yard.val.predict) # MAE

mae(val$kickReturnYardage, val.unscale.predictions)
#5.258 with scaled target on Liam's variables
#5.69 without scaling target on Liam's variables

val.preds.nn = cbind(val, val.unscale.predictions)
```


############################# Combining XGBoost and NN ################################
```{r}
val.preds.all = merge(val.preds.xgb, val.preds.nn[c(2,3,76)], by = c("gameId", "playId"))

mae(val.preds.all$kickReturnYardage, (val.preds.all$xgb_half_pred + val.preds.all$val.unscale.predictions )/2)

#lm(kickReturnYardage ~ xgb_half_pred + val.unscale.predictions, data = train.preds.all)

```



#################### Applying model to determine best returner/teams ###################
#Based on Areas
```{r}
full.h1 = full_set %>% filter(return_x < 28)
full.h2 = full_set %>% filter(return_x >= 28)


half_xgbs = list(xgb.punt.h1, xgb.punt.h2)

full_with_preds = data.frame()
for (i in 1:2){
  if (i == 1) {
      full.h = full %>% filter(return_x < 28)
    }
    else {
      full.h = full %>% filter(return_x >= 28)
    }
  
  full_x = model.matrix(kickReturnYardage ~ ., data = full.h[,c(4:6, 8:40,63:68, 70, 71 )])[,-1]
  full_y = full.h$kickReturnYardage
  
  full.h$xgb_area_pred = predict(half_xgbs[i], type = "response", full_x)[[1]]
  full_with_preds = rbind(full.h, full_with_preds)

}

```

```{r}
full_with_preds$yard_dif = full_with_preds$kickReturnYardage - full_with_preds$xgb_half_pred

full_with_preds %>% group_by(returnerId) %>% summarise(yards_over_expected = mean(yard_dif)) %>% arrange(desc(yards_over_expected))


```











####################################### Archive ########################################

#Create different data sets depending on zone groups
```{r}
train.a1 = train %>% filter(return_zone %in% c(1,2,3))
train.a2 = train %>% filter(return_zone %in% c(4,5,6))
train.a3 = train %>% filter(return_zone %in% c(7,8,9))

val.a1 = val %>% filter(return_zone %in% c(1,2,3))
val.a2 = val %>% filter(return_zone %in% c(4,5,6))
val.a3 = val %>% filter(return_zone %in% c(7,8,9))
```


```{r}
varlist = list(c(4:6, 8:40,63, 65,67:71 ),c(4:6, 8:40,63, 64,67:71 ),
               c(4:6, 8:40,52:71 ), c(4:6, 8:40,52:65, 67:71 ), c(4:6, 8:40,63:71 ),
               c(4:40,63:71 ), c(4:6, 8:40,63,64,66:71 ), c(5:6, 8:40,63:71 ), 
               c(4:6, 8:40,63:67, 69:71 ), c(4:6, 8:40,63:68, 70, 71 ), 
               c(4:6, 8:40,63:66, 68:71 ), c(4:6, 8:40,63,64,68:71 ))


num = 0
#for (vars in varlist){
#Placeholder loop if not looping through different groups of variables
for (j in 1) {
  num = num + 1
  for (i in 1:3){
    print(paste("Area ", i))
    zones = ifelse(i == 1, list(1:3), ifelse(i == 2, list(4:6), list(7:9)))
    train.a = train %>% filter(return_zone %in% zones[[1]])
  
    #No sides or zones of defenders included
    train_x = model.matrix(kickReturnYardage ~ ., data = train.a[,c(4:6, 8:40,63:68, 70, 71 )])[,-1]
    train_y = train.a$kickReturnYardage
  
    tune_grid = expand.grid(
    nrounds = 4,
    eta = c(.1, .15, .2, .25, .3),
    max_depth = c(1:10),
    gamma = c(0),
    colsample_bytree = 1,
    min_child_weight = 1,
    subsample = c(.25, .5, .75, 1)
    )

    set.seed(123)
    xgb.punt.caret = train(x = train_x, y = train_y, method = "xgbTree", tuneGrid = tune_grid, trControl = trainControl(method = 'cv', number = 10), metric = 'MAE')

    set.seed(123)
    assign(paste0("xgb.punt.a", i), xgboost(data = train_x, label = train_y, 
                           subsample = xgb.punt.caret$bestTune$subsample, nrounds = 4, 
                           eta = xgb.punt.caret$bestTune$eta,
                           max_depth = xgb.punt.caret$bestTune$max_depth, 
                           prediction = T, eval_metric = 'mae'))

    set.seed(123)
    xgb.punt.a = xgboost(data = train_x, label = train_y, 
                           subsample = xgb.punt.caret$bestTune$subsample, nrounds = 4, 
                           eta = xgb.punt.caret$bestTune$eta,
                           max_depth = xgb.punt.caret$bestTune$max_depth, 
                           prediction = T, eval_metric = 'mae')


    #x = xgb.importance(feature_names = colnames(train_x), model = xgb.punt.final)
    #print(x)
    
    g = xgb.ggplot.importance(xgb.importance(feature_names = colnames(train_x), model = xgb.punt.a)) + ggtitle(paste("Feature Importance Area", i))
    print(g)
  }

  averaged_mae = (min(xgb.punt.a1$evaluation_log$train_mae)*nrow(train.a1) +
      min(xgb.punt.a2$evaluation_log$train_mae)*nrow(train.a2) +
      min(xgb.punt.a3$evaluation_log$train_mae)*nrow(train.a3))/nrow(train)
  
  print(num)
  print(vars)
  print(averaged_mae)
}
for (i in 1:3){
    print(paste("Area ", i))
    
    zones = ifelse(i == 1, list(1:3), ifelse(i == 2, list(4:6), list(7:9)))
    train.a = train %>% filter(return_zone %in% zones[[1]])
    print(zones)
    print(nrow(train.a))
}

#1 = 5.180
#2 = 5.374
#3 = 5.282
#4 = 5.285
#5 = 5.208
#6 = 5.208
#7 = 5.369
#8 = 5.210
#9 = 5.206
#10 = 5.084
#11 = 5.212
#12 = 5.367

#Weighted MAE of 5.084 best for variables: c(4:6, 8:40,63:68, 70, 71 )
#Weighted MAE of 5.180 best for variables: c(4:6, 8:40,63, 65,67:71 )
### If we can't include actual return direction
#Weighted MAE of 5.367 best for variables: c(4:6, 8:40,63,64,68:71 )
train %>% filter(return_x < 32)
```

#Run area xgboost on validation
```{r}
area_xgbs = list(xgb.punt.a1, xgb.punt.a2, xgb.punt.a3)

for (i in 1:3){
  zones = ifelse(i == 1, list(1:3), ifelse(i == 2, list(4:6), list(7:9)))
  val.a = val %>% filter(return_zone %in% zones[[1]])
  
  val_x = model.matrix(kickReturnYardage ~ ., data = val.a[,c(4:6, 8:40,63:68, 70, 71 )])[,-1]
  val_y = val.a$kickReturnYardage
  
  val.a$xgb_area_pred = predict(area_xgbs[i], type = "response", val_x)[[1]]
  
  assign(paste0("val.mae.a", i), MAE(val.a$xgb_area_pred, val.a$kickReturnYardage))
}


(val.mae.a1*nrow(val.a1) + val.mae.a2*nrow(val.a2) + val.mae.a3*nrow(val.a3))/nrow(val)

#Weighted Val MAE of 5.199 for c(4:6, 8:40,63:68, 70, 71 )

```


############################## XGBoost by Zone ###########################
#Create different data sets depending on zone
```{r}
train.z1 = train %>% filter(return_zone == 1)
train.z2 = train %>% filter(return_zone == 2)
train.z3 = train %>% filter(return_zone == 3)
train.z4 = train %>% filter(return_zone == 4)
train.z5 = train %>% filter(return_zone == 5)
train.z6 = train %>% filter(return_zone == 6)
train.z7 = train %>% filter(return_zone == 7)
train.z8 = train %>% filter(return_zone == 8)
train.z9 = train %>% filter(return_zone == 9)

val.z1 = val %>% filter(return_zone == 1)
val.z2 = val %>% filter(return_zone == 2)
val.z3 = val %>% filter(return_zone == 3)
val.z4 = val %>% filter(return_zone == 4)
val.z5 = val %>% filter(return_zone == 5)
val.z6 = val %>% filter(return_zone == 6)
val.z7 = val %>% filter(return_zone == 7)
val.z8 = val %>% filter(return_zone == 8)
val.z9 = val %>% filter(return_zone == 9)

```


```{r}
varlist = list(c(4:6, 8:40,52:66,68:71 ), c(4:6, 8:40,63:65, 68:71 ),
               c(4:6, 8:40,63, 65,68:71 ),c(4:6, 8:40,63, 64,68:71 ),
               c(4:6, 8:40,52:63, 65,68:71 ),c(4:6, 8:40,52:64,68:71 ),
               c(4:6, 8:40,63, 65, 66,68:71 ),c(4:6, 8:40,63, 64, 66,68:71 ),
               c(4:6, 8:40,52:63, 65, 66,68:71 ),c(4:6, 8:40,52:64, 66,68:71 ))


num = 0
#for (vars in varlist){
#Placeholder loop if not looping through different groups of variables
for (j in 1) {
  num = num + 1
  for (i in 1:9){
    print(paste("Zone ", i))
    train.z = train %>% filter(return_zone == i)
    train_x = model.matrix(kickReturnYardage ~ ., data = train.z[,c(4:6, 8:40,52:64,68:71 )])[,-1]
    train_y = train.z$kickReturnYardage
  
    tune_grid = expand.grid(
    nrounds = 4,
    eta = c(.1, .15, .2, .25, .3),
    max_depth = c(1:10),
    gamma = c(0),
    colsample_bytree = 1,
    min_child_weight = 1,
    subsample = c(.25, .5, .75, 1)
    )

    set.seed(123)
    xgb.punt.caret = train(x = train_x, y = train_y, method = "xgbTree", tuneGrid = tune_grid, trControl = trainControl(method = 'cv', number = 10), metric = 'MAE')

    set.seed(123)
    assign(paste0("xgb.punt.z", i), xgboost(data = train_x, label = train_y, 
                           subsample = xgb.punt.caret$bestTune$subsample, nrounds = 4, 
                           eta = xgb.punt.caret$bestTune$eta,
                           max_depth = xgb.punt.caret$bestTune$max_depth, 
                           prediction = T, eval_metric = 'mae'))

    set.seed(123)
    xgb.punt.z = xgboost(data = train_x, label = train_y, 
                           subsample = xgb.punt.caret$bestTune$subsample, nrounds = 4, 
                           eta = xgb.punt.caret$bestTune$eta,
                           max_depth = xgb.punt.caret$bestTune$max_depth, 
                           prediction = T, eval_metric = 'mae')


    #x = xgb.importance(feature_names = colnames(train_x), model = xgb.punt.final)
    #print(x)
    #g = xgb.ggplot.importance(xgb.importance(feature_names = colnames(train_x), model = xgb.punt.z)) + ggtitle(paste("Feature Importance Zone", i))
    #print(g)
  }

  averaged_mae = (min(xgb.punt.z1$evaluation_log$train_mae)*nrow(train.z1) + min(xgb.punt.z2$evaluation_log$train_mae)*nrow(train.z2) + min(xgb.punt.z3$evaluation_log$train_mae)*nrow(train.z3) + min(xgb.punt.z4$evaluation_log$train_mae)*nrow(train.z4) + min(xgb.punt.z5$evaluation_log$train_mae)*nrow(train.z5) + min(xgb.punt.z6$evaluation_log$train_mae)*nrow(train.z6) + min(xgb.punt.z7$evaluation_log$train_mae)*nrow(train.z7) + min(xgb.punt.z8$evaluation_log$train_mae)*nrow(train.z8) + min(xgb.punt.z9$evaluation_log$train_mae)*nrow(train.z9))/nrow(train)

  print(num)
  print(vars)
  print(averaged_mae)
}


#2 = 5.072
#3 = 5.197
#4 = 5.350
#5 = 5.368
#6 = 4.863
#7 = 5.245
#8 = 5.534
#9 = 5.363
#10 = 4.863
#xgb.punt.z6$feature_names

### Best overall
#Weighted MAE of 4.863 best for variables: c(4:6, 8:40,52:64,68:71 )
### Best that didn't include sides
#Weighted MAE of 5.072 best for variables: c(4:6, 8:40,63:65, 68:71 ) 
### If we can't include intended return direction
#Weighted MAE of 5.197 best for variables: c(4:6, 8:40,63, 65,68:71 )
### If we can't include actual return direction and the defensive player sides
#Weighted MAE of 5.350 best for variables: c(4:6, 8:40,63, 64,68:71 ) 
```




#Run zone xgboost on validation
```{r}
zone_xgbs = list(xgb.punt.z1, xgb.punt.z2, xgb.punt.z3, xgb.punt.z4, xgb.punt.z5, xgb.punt.z6, xgb.punt.z7, xgb.punt.z8, xgb.punt.z9)

for (i in 1:9){
  val.z = val %>% filter(return_zone == i)
  if (nrow(val.z) > 0) {
    val_x = model.matrix(kickReturnYardage ~ ., data = val.z[,c(4:6, 8:40,52:64,68:71 ) ])[,-1]
    val_y = val.z$kickReturnYardage
  
    val.z$xgb_zone_pred = predict(zone_xgbs[i], type = "response", val_x)[[1]]
  
    assign(paste0("val.mae.z", i), MAE(val.z$xgb_zone_pred, val.z$kickReturnYardage))
  }
  else{
    assign(paste0("val.mae.z", i), 0)
  }
}


(val.mae.z1*nrow(val.z1) + val.mae.z2*nrow(val.z2) + val.mae.z3*nrow(val.z3) +
  val.mae.z4*nrow(val.z4) + val.mae.z5*nrow(val.z5) + val.mae.z6*nrow(val.z6) +
  val.mae.z7*nrow(val.z7) + val.mae.z8*nrow(val.z8) + val.mae.z9*nrow(val.z9))/nrow(val)

#Weighted Val MAE of 5.516 for c(4:6, 8:40,52:64,68:71 )
#Weighted Val MAE of 5.687 for c(4:6, 8:40,63:65, 68:71 ) 
summary(shared$avg_epa)
```






