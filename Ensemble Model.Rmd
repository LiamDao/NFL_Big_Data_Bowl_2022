---
title: "Final Ensemble Model"
author: "Trevor Hughes"
date: "12/19/2021"
output: html_document
---

```{r}
library(dplyr)
library(smbinning)
library(corpcor)
library(car)
library(rpart)
library(rpart.plot)
library(ggplot2)
library(xgboost)
library(caret)
library(Ckmeans.1d.dp)
library(nnet)
library(NeuralNetTools)
library(Metrics)

# Create scaling function for variable importance
scale_01 <- function(x) {                              
  (x - min(x)) / (max(x) - min(x))
}
```

#read in data
```{r}
model_set = read.csv("C:\\Users\\thughes\\Documents\\model set.csv")
train = read.csv("C:\\Users\\thughes\\Documents\\model train set.csv")
val = read.csv("C:\\Users\\thughes\\Documents\\model val set.csv")
full_set = read.csv("C:\\Users\\thughes\\Documents\\full set.csv")

```

```{r}
#Change all variables with less than 16 unique values to factor
for (i in 1:length(train)) {
  if (length(unique(train[,i])) < 16) {
    train[,i] = as.factor(train[,i])
  }
}

train = train %>% filter(is.na(train$kickReturnYardage) == 0)

for (i in 1:length(val)) {
  if (length(unique(val[,i])) < 16) {
    val[,i] = as.factor(val[,i])
  }
}
  
val = val %>% filter(is.na(val$kickReturnYardage) == 0)

for (i in 1:length(full_set)) {
  if (length(unique(full_set[,i])) < 16) {
    full_set[,i] = as.factor(full_set[,i])
  }
}
  
full_set = full_set %>% filter(is.na(full_set$kickReturnYardage) == 0)
```



######################################### XGBoost ######################################
```{r}
train.h1 = train %>% filter(return_x < 28)
train.h2 = train %>% filter(return_x >= 28)

levels(val$return_zone) = c(levels(val$return_zone), "9")
val.h1 = val %>% filter(return_x < 28)
val.h2 = val %>% filter(return_x >= 28)

```


```{r}

for (i in 1:2){
  print(paste("Half ", i))
    
  if (i == 1) {
    train.h = train %>% filter(return_x < 28)
  }
  else {
    train.h = train %>% filter(return_x >= 28)
  }
  #No sides or zones of defenders included
  train_x = model.matrix(kickReturnYardage ~ ., 
                         data = train.h[,c(5, 8:40,63,64,66:71 )])[,-1]
  train_y = train.h$kickReturnYardage
  
  tune_grid = expand.grid(
    nrounds = 4,
    eta = c(.1, .15, .2, .25, .3),
    max_depth = c(1:10),
    gamma = c(0),
    colsample_bytree = 1,
    min_child_weight = 1,
    subsample = c(.25, .5, .75, 1)
  )

  set.seed(123)
  xgb.punt.caret = train(x = train_x, y = train_y, method = "xgbTree", 
                         tuneGrid = tune_grid, 
                         trControl = trainControl(method = 'cv', number = 10),
                         metric = 'MAE')

  set.seed(123)
  assign(paste0("xgb.punt.h", i), xgboost(data = train_x, label = train_y, 
                         subsample = xgb.punt.caret$bestTune$subsample, nrounds = 4, 
                         eta = xgb.punt.caret$bestTune$eta,
                         max_depth = xgb.punt.caret$bestTune$max_depth, 
                         prediction = T, eval_metric = 'mae'))

  set.seed(123)
  xgb.punt.h = xgboost(data = train_x, label = train_y, 
                         subsample = xgb.punt.caret$bestTune$subsample, nrounds = 4, 
                         eta = xgb.punt.caret$bestTune$eta,
                         max_depth = xgb.punt.caret$bestTune$max_depth, 
                         prediction = T, eval_metric = 'mae')


  assign(paste0("imp.h", i), xgb.importance(feature_names = colnames(train_x), model = xgb.punt.h))
  
    
  g = xgb.ggplot.importance(xgb.importance(feature_names = colnames(train_x),
                                           model = xgb.punt.h)) + 
    ggtitle(paste("Feature Importance Half", i))
  print(g)
  
  assign(paste0("preds", i), 
                predict(xgb.punt.h, type = "response", train_x))
}


train.h1$XGBpreds = preds1
train.h2$XGBpreds = preds2
train.preds = rbind(train.h1, train.h2)
mae(train.preds$XGBpreds, train.preds$kickReturnYardage)

#c(5, 8:40,63,64,66:71 ) MAE of 4.729

#Variable importance
imp.h = merge(imp.h1[,c(1,2)], imp.h2[,c(1,2)], by = "Feature", all = TRUE)
imp.h$Gain.x = ifelse(is.na(imp.h$Gain.x), 0, imp.h$Gain.x)
imp.h$Gain.y = ifelse(is.na(imp.h$Gain.y), 0, imp.h$Gain.y)

imp.h$Gain = (imp.h$Gain.x*length(train.h1) + imp.h$Gain.y*length(train.h2))/length(train)

imp.h$scaledImpHalf = scale_01(imp.h$Gain)
imp.h = imp.h %>% arrange(desc(scaledImpHalf))
```


#Run halves xgboost on validation
```{r}
half_xgbs = list(xgb.punt.h1, xgb.punt.h2)
val.preds.h1 = data.frame()
val.preds.h2 = data.frame()

for (i in 1:2){
  if (i == 1) {
      val.h = val %>% filter(return_x < 28)
    }
    else {
      val.h = val %>% filter(return_x >= 28)
    }
  
  val_x = model.matrix(kickReturnYardage ~ ., data = val.h[,c(5, 8:40,63,64,66:71 )])[,-1]
  val_y = val.h$kickReturnYardage
  
  val.h$XGBpreds = predict(half_xgbs[i], type = "response", val_x)[[1]]
  
  assign(paste0("val.mae.h", i), MAE(val.h$XGBpreds, val.h$kickReturnYardage))
  
  #Storing predictions for ensemble model
  assign(paste0("val.preds.h",i), val.h)
}



(val.mae.h1*nrow(val.h1) + val.mae.h2*nrow(val.h2))/nrow(val)

#Weighted Val MAE of 5.314 for c(5, 8:40,63,64,66:71 ) 

#Combine predictions into one data set for ensemble
val.preds.xgb = rbind(val.preds.h1, val.preds.h2)
```


#################################### Neural Network ####################################


```{r}
#Getting variable importance from decision tree
fit = rpart(train$kickReturnYardage ~ . , data = train[,c(4:40,52:64,66:71)])
rpart.plot(fit)
varimp.data = data.frame(fit$variable.importance)
varimp.data$names = as.character(rownames(varimp.data))
varimp.data


train2 = train
val2 = val

# Scale standardize 
trainScale = preProcess(train2[4:71])
train2 = cbind( train2[1:3], predict(trainScale, train2[4:71]))
val2 = cbind( val2[1:3], predict(trainScale, val2[4:71]))



# Check columns data types
sapply(train2, class) # Dataset contains numeric, integer, and factor variables

# Using varimp.data output. Cutoff of 700 for fit.variable.importance
var.sel <- varimp.data$names

train2 <- train2 %>% 
  select(var.sel, kickReturnYardage, X, gameId, playId)

val2 <- val2 %>% 
  select(var.sel, X, kickReturnYardage, gameId, playId)

#Create Neural Network
set.seed(123)
nn.yard.model <- nnet(kickReturnYardage ~ . -gameId -playId -X, data = train2, size = 6, decay = .945, linout = TRUE )

#Add unscaled predictions to train.preds df
train2$NNpreds = sd(train$kickReturnYardage, na.rm = TRUE) * predict(nn.yard.model, train2)+ mean(train$kickReturnYardage, na.rm = TRUE)
train.preds = merge(train.preds, train2[c(21:23)],by = c("gameId", "playId") )
mae(train.preds$kickReturnYardage, train.preds$NNpreds)
#MAE of 5.255 on train

nn.yard.val.predict <- predict(nn.yard.model, newdata = val2)

# Calculate RMSE and MAE on scaled target
val.rmse <- sqrt(mean((nn.yard.val.predict - val2$kickReturnYardage)^2)) # RMSE
val.mae <- mae(val2$kickReturnYardage, nn.yard.val.predict) # MAE

#Since predictions are on a scaled target variable, must unscale the predictions
val.preds.nn = val
val.preds.nn$NNpreds = sd(train$kickReturnYardage, na.rm = TRUE) * nn.yard.val.predict + mean(train$kickReturnYardage, na.rm = TRUE)

mae(val$kickReturnYardage, val.preds.nn$NNpreds)
#MAE of 5.258 on val after unscaling predictions

#Variable importance
imp.nn.garson = garson(nn.yard.model)
imp.nn = data.frame(cbind(Feature = levels(imp.nn.garson$data$x_names) ,GarsonImp = imp.nn.garson$data$rel_imp))

imp.nn$scaledImpNN = scale_01(as.numeric(imp.nn$GarsonImp))
```


############################ Pranav's Offensive Model #################################


```{r}
#Included only listed variables
train_x = model.matrix(kickReturnYardage ~ ., 
                       data = train_off[,c(3:36,92,93,95:100 )])[,-1]
train_y = train_off$kickReturnYardage

# Build XGBoost model
set.seed(123)
#xgb <- xgb.cv(data = train_x, label = train_y, subsample = 0.5, nrounds = 100, nfold = 10, eval.metric = 'mae')
#[4]	test MAE lowest at nrounds = 4

## Caret Tuning
#set seed
set.seed(123)
# Tuning through caret
tune_grid = expand.grid(
    nrounds = 5,
    eta = c(.1, .15, .2, .25, .3),
    max_depth = c(1:10),
    gamma = c(0),
    colsample_bytree = 1,
    min_child_weight = 1,
    subsample = c(.25, .5, .75, 1))

#set seed
set.seed(123)
#build caret tuning
xgb.punt.caret = train(x = train_x, y = train_y, method = "xgbTree", 
                         tuneGrid = tune_grid, 
                         trControl = trainControl(method = 'cv', number = 10),
                         metric = 'MAE')

#plot tuning grid
plot(xgb.punt.caret)
#4 rounds
#0.3 eta
#subsample of 1.00

#xgb.punt.jamaica <- xgb.cv(data = train_x, label = train_y, subsample = 1, nrounds = 100, nfold = 10, eta = 0.3, max_depth = 4, eval.metric = 'mae')
#best at 6?

############################################################################################################
xgb.punt.jamaica <-  xgboost(data = train_x, label = train_y, 
                         subsample = xgb.punt.caret$bestTune$subsample, nrounds = 5, 
                         eta = xgb.punt.caret$bestTune$eta,
                         max_depth = xgb.punt.caret$bestTune$max_depth, 
                         prediction = T, eval_metric = 'mae')
#MAE = 4.796


#xgb.punt.jamaica <- xgboost(data = train_x, label = train_y, subsample = 1, nrounds = 6, eta = 0.3, max_depth = 4, eval.metric = 'mae')
#MAE = 4.838582
############################################################################################################

train.off.preds = train_off
val.off.preds = val_off

train.off.preds$OFFpreds = predict(xgb.punt.jamaica, type = "response", train_x)
train.preds = merge(train.preds, train.off.preds[c(1,2,101)], by = c("gameId", "playId"))

levels(val_off$return_zone) = c(levels(val_off$return_zone), "9")
val_x = model.matrix(kickReturnYardage ~ ., data = val_off[,c(3:36,92,93,95:100 )])[,-1]
val_y = val_off$kickReturnYardage

val.off.preds$OFFpreds = predict(xgb.punt.jamaica, type = "response", val_x)
MAE(val.off.preds$OFFpreds, val.off.preds$kickReturnYardage)
#Val MAE of 6.294

#Variable Importance
imp.off = xgb.importance(feature_names = colnames(train_x), model = xgb.punt.jamaica)
imp.off$scaledImpOff = scale_01(imp.off$Gain)

```

############################### Combining the 3 models #################################
```{r}
val.preds = merge(val.preds.xgb, val.preds.nn[c(2,3,76)], by = c("gameId", "playId"))
val.preds = merge(val.preds, val.off.preds[c(1,2,101)], by = c("gameId", "playId"))

#Simple average
mae(train.preds$kickReturnYardage, (train.preds$XGBpreds + train.preds$NNpreds + train.preds$OFFpreds)/3)
#MAE of 4.550
mae(val.preds$kickReturnYardage, (val.preds$XGBpreds + val.preds$NNpreds + val.preds$OFFpreds)/3)
#MAE of 3.643


#Linear model ensemble
lm.ensemble = lm(kickReturnYardage ~ XGBpreds + NNpreds + OFFpreds, data = train.preds)
lm.ensemble
train.preds$ENS_LMpreds = predict(lm.ensemble, train.preds)
val.preds$ENS_LMpreds = predict(lm.ensemble, val.preds)

mae(train.preds$kickReturnYardage, train.preds$ENS_LMpreds)
#MAE of 4.021
mae(val.preds$kickReturnYardage, val.preds$ENS_LMpreds)
#MAE of 5.189


#Neural Network Ensemble
nn.ensemble <- nnet(kickReturnYardage ~ NNpreds + XGBpreds + OFFpreds, data = train.preds, size = 1, linout = TRUE )

train.preds$ENS_NNpreds = predict(nn.ensemble, train.preds)
val.preds$ENS_NNpreds = predict(nn.ensemble, val.preds)

mae(train.preds$kickReturnYardage, train.preds$ENS_NNpreds)
#MAE of 3.9168 on train
mae(val.preds$kickReturnYardage, val.preds$ENS_NNpreds)
#MAE of 5.314 on val
```

############################ Combined Variable Importance ##############################
```{r}
#Join importance metrics
imp.ensemble = full_join(imp.h[,c(1,5)],imp.off[,c(1,5)] , by = "Feature", all = TRUE)
imp.ensemble = full_join(imp.ensemble, imp.nn[,c(1,3)], by = "Feature", all = TRUE)
imp.ensemble$Feature == "returnDirectionIntendedR"

#Set null values to 0 to allow addition
imp.ensemble$scaledImpHalf = ifelse(is.na(imp.ensemble$scaledImpHalf), 0, imp.ensemble$scaledImpHalf)
imp.ensemble$scaledImpNN = ifelse(is.na(imp.ensemble$scaledImpNN), 0, imp.ensemble$scaledImpNN)
imp.ensemble$scaledImpOff = ifelse(is.na(imp.ensemble$scaledImpOff), 0, imp.ensemble$scaledImpOff)

#Find average of importance metrics and sort in descending order
imp.ensemble$avgScaledImp = (imp.ensemble$scaledImpHalf + imp.ensemble$scaledImpNN + imp.ensemble$scaledImpOff)/3
imp.ensemble = imp.ensemble %>% arrange(desc(avgScaledImp))


```


#################### Applying model to determine best returner/teams ###################


```{r}
#Calculate XGB predictions on full data set
full.h1 = full_set %>% filter(return_x < 28)
full.h2 = full_set %>% filter(return_x >= 28)


half_xgbs = list(xgb.punt.h1, xgb.punt.h2)

full.preds = data.frame()
for (i in 1:2){
  if (i == 1) {
      full.h = full_set %>% filter(return_x < 28)
    }
    else {
      full.h = full_set %>% filter(return_x >= 28)
    }
  
  full_x = model.matrix(kickReturnYardage ~ ., data = full.h[,c(5, 8:40,63,64,66:71 )])[,-1]
  full_y = full.h$kickReturnYardage
  
  full.h$XGBpreds = predict(half_xgbs[i], type = "response", full_x)[[1]]
  full.preds = rbind(full.h, full.preds)

}



#Calculate NN predictions on full data set

full2 = cbind( full_set[1:3], predict(trainScale, full_set[4:71]))

full2 <- full2 %>% 
  select(var.sel, X, kickReturnYardage, gameId, playId)

nn.yard.full.predict <- predict(nn.yard.model, newdata = full2)
full2$NNpreds = sd(train$kickReturnYardage, na.rm = TRUE) * nn.yard.full.predict + mean(train$kickReturnYardage, na.rm = TRUE)

#Combine both full sets
full.preds = merge(full.preds, full2[21:23], by = c("gameId", "playId"))


```



```{r}
full.preds$ENS_AVGpreds = (full.preds$NNpreds + full.preds$XGBpreds)/2
full.preds$yard_dif = full.preds$kickReturnYardage - full.preds$ENS_AVGpreds

#Find best & worst returners
full.preds %>% group_by(returnerId, punt_returner_player_name) %>% summarise(yards_over_expected = mean(yard_dif)) %>% arrange(desc(yards_over_expected))

#Find best & worst teams with getting in position
full.preds %>% group_by(possessionTeam) %>% summarise(yards_expected = mean(ENS_AVGpreds)) %>% arrange(desc(yards_expected))

#Find best & worst teams with punt coverage compared to expected
full.preds %>% group_by(possessionTeam) %>% summarise(yards_over_expected = mean(yard_dif)) %>% arrange(desc(yards_over_expected))


```
