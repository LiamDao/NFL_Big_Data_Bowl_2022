---
title: "Final Ensemble Model"
author: "Trevor Hughes"
date: "12/19/2021"
output: html_document
---

```{r}
library(dplyr)
library(smbinning)
library(corpcor)
library(car)
library(rpart)
library(rpart.plot)
library(ggplot2)
library(xgboost)
library(caret)
library(Ckmeans.1d.dp)
library(nnet)
library(NeuralNetTools)
library(Metrics)

# Create scaling function for variable importance
scale_01 <- function(x) {                              
  (x - min(x)) / (max(x) - min(x))
}
```

#read in data and create train, val, and test sets
```{r}
final_set = read.csv("C:\\Users\\thughes\\Documents\\final set.csv")
full_set = read.csv("C:\\Users\\thughes\\Documents\\full set.csv")

final_set = final_set %>% select(-"X")
full_set = full_set %>% select(-"X")

#Change all variables with less than 16 unique values to factor
for (i in 1:length(final_set)) {
  if (length(unique(final_set[,i])) < 16) {
    final_set[,i] = as.factor(final_set[,i])
  }
}

for (i in 1:length(full_set)) {
  if (length(unique(full_set[,i])) < 16) {
    full_set[,i] = as.factor(full_set[,i])
  }
}


#Split into train, val, and test
set.seed(123)
train = final_set %>% sample_frac(.7)
non_train = anti_join(final_set, train, by = c('gameId', 'playId'))
set.seed(123)
val = non_train %>% sample_frac(2/3)
test = anti_join(non_train, val, by = c('gameId', 'playId'))

#Remove missing values
final_set = final_set %>% filter(is.na(final_set$kickReturnYardage) == 0)
train = train %>% filter(is.na(train$kickReturnYardage) == 0)
val = val %>% filter(is.na(val$kickReturnYardage) == 0)
test = test %>% filter(is.na(test$kickReturnYardage) == 0)

write.csv(train, "C:\\Users\\thughes\\Documents\\model train set.csv")
write.csv(val, "C:\\Users\\thughes\\Documents\\model val set.csv")
write.csv(test, "C:\\Users\\thughes\\Documents\\model test set.csv")
```


################################## Defense XGBoost #####################################
```{r}
#Create decision tree of yardlineTotal to see where significant splits are

fit = rpart(kickReturnYardage ~ return_x , data = train, cp = .0025, minbucket = 50)
#rpart.plot(fit)

#Split data at that part of the field
train.h1 = train %>% filter(return_x < 28)
train.h2 = train %>% filter(return_x >= 28)

#levels(val$return_zone) = c(levels(train$return_zone), "9")
val.h1 = val %>% filter(return_x < 28)
val.h2 = val %>% filter(return_x >= 28)

test.h1 = test %>% filter(return_x < 28)
test.h2 = test %>% filter(return_x >= 28)
```


```{r}

for (i in 1:2){
  print(paste("Half ", i))
    
  if (i == 1) {
    train.h = train %>% filter(return_x < 28)
  }
  else {
    train.h = train %>% filter(return_x >= 28)
  }
  #No sides or zones of defenders included
  train_x = model.matrix(kickReturnYardage ~ ., 
                         data = train.h[,c(3:5, 7:39,62,63,65:70 )])[,-1]
  train_y = train.h$kickReturnYardage
  
  tune_grid = expand.grid(
    nrounds = 4,
    eta = c(.1, .15, .2, .25, .3),
    max_depth = c(1:10),
    gamma = c(0),
    colsample_bytree = 1,
    min_child_weight = 1,
    subsample = c(.25, .5, .75, 1)
  )

  set.seed(123)
  xgb.punt.caret = train(x = train_x, y = train_y, method = "xgbTree", 
                         tuneGrid = tune_grid, 
                         trControl = trainControl(method = 'cv', number = 10),
                         metric = 'MAE')

  set.seed(123)
  assign(paste0("xgb.punt.h", i), xgboost(data = train_x, label = train_y, 
                         subsample = xgb.punt.caret$bestTune$subsample, nrounds = 4, 
                         eta = xgb.punt.caret$bestTune$eta,
                         max_depth = xgb.punt.caret$bestTune$max_depth, 
                         prediction = T, eval_metric = 'mae'))

  set.seed(123)
  xgb.punt.h = xgboost(data = train_x, label = train_y, 
                         subsample = xgb.punt.caret$bestTune$subsample, nrounds = 4, 
                         eta = xgb.punt.caret$bestTune$eta,
                         max_depth = xgb.punt.caret$bestTune$max_depth, 
                         prediction = T, eval_metric = 'mae')


  assign(paste0("imp.h", i), xgb.importance(feature_names = colnames(train_x), model = xgb.punt.h))
  
    
  g = xgb.ggplot.importance(xgb.importance(feature_names = colnames(train_x),
                                           model = xgb.punt.h)) + 
    ggtitle(paste("Feature Importance Half", i))
  print(g)
  
  assign(paste0("preds", i), 
                predict(xgb.punt.h, type = "response", train_x))
}


train.h1$XGBpreds = preds1
train.h2$XGBpreds = preds2
train.preds = rbind(train.h1, train.h2)
mae(train.preds$XGBpreds, train.preds$kickReturnYardage)

#Train MAE of 4.835


#Variable importance
imp.h = merge(imp.h1[,c(1,2)], imp.h2[,c(1,2)], by = "Feature", all = TRUE)
imp.h$Gain.x = ifelse(is.na(imp.h$Gain.x), 0, imp.h$Gain.x)
imp.h$Gain.y = ifelse(is.na(imp.h$Gain.y), 0, imp.h$Gain.y)

imp.h$Gain = (imp.h$Gain.x*length(train.h1) + imp.h$Gain.y*length(train.h2))/length(train)

imp.h$scaledImpHalf = scale_01(imp.h$Gain)
imp.h = imp.h %>% arrange(desc(scaledImpHalf))
```


#Run halves xgboost on validation 
```{r}
half_xgbs = list(xgb.punt.h1, xgb.punt.h2)
val.preds.h1 = data.frame()
val.preds.h2 = data.frame()

for (i in 1:2){
  if (i == 1) {
      val.h = val %>% filter(return_x < 28)
    }
    else {
      val.h = val %>% filter(return_x >= 28)
    }
  
  val_x = model.matrix(kickReturnYardage ~ ., data = val.h[,c(3:5, 7:39,62,63,65:70 )])[,-1]
  val_y = val.h$kickReturnYardage
  
  val.h$XGBpreds = predict(half_xgbs[i], type = "response", val_x)[[1]]
  
  assign(paste0("val.mae.h", i), MAE(val.h$XGBpreds, val.h$kickReturnYardage))
  
  #Storing predictions for ensemble model
  assign(paste0("val.preds.h",i), val.h)
}

(val.mae.h1*nrow(val.h1) + val.mae.h2*nrow(val.h2))/nrow(val)
#Weighted Val MAE of 6.455


#Combine predictions into one data set for ensemble
val.preds.xgb = rbind(val.preds.h1, val.preds.h2)

```


#################################### Neural Network ####################################


```{r}
#Getting variable importance from decision tree
fit = rpart(train$kickReturnYardage ~ . , data = train[,c(3:39,51:63,65:104)])
rpart.plot(fit)
varimp.data = data.frame(fit$variable.importance)
varimp.data$names = as.character(rownames(varimp.data))
varimp.data


train2 = train
val2 = val

# Scale standardize 
trainScale = preProcess(train2[3:104])
train2 = cbind( train2[1:2], predict(trainScale, train2[3:104]))
val2 = cbind( val2[1:2], predict(trainScale, val2[3:104]))


# Using varimp.data output. Cutoff of 700 for fit.variable.importance
var.sel <- varimp.data$names

train2 <- train2 %>% 
  select(var.sel, kickReturnYardage, gameId, playId)

val2 <- val2 %>% 
  select(var.sel, kickReturnYardage, gameId, playId)

#Create Neural Network
set.seed(123)
nn.yard.model <- nnet(kickReturnYardage ~ . -gameId -playId , data = train2, size = 6, decay = .945, linout = TRUE )

#Add unscaled predictions to train.preds df
train2$NNpreds = sd(train$kickReturnYardage, na.rm = TRUE) * predict(nn.yard.model, train2)+ mean(train$kickReturnYardage, na.rm = TRUE)
train.preds = merge(train.preds, train2[c(37:39)],by = c("gameId", "playId") )
mae(train.preds$kickReturnYardage, train.preds$NNpreds)
#MAE of 5.155 on train


nn.yard.val.predict <- predict(nn.yard.model, newdata = val2)

# Calculate RMSE and MAE on scaled target
val.rmse <- sqrt(mean((nn.yard.val.predict - val2$kickReturnYardage)^2)) # RMSE
val.mae <- mae(val2$kickReturnYardage, nn.yard.val.predict) # MAE

#Since predictions are on a scaled target variable, must unscale the predictions
val.preds.nn = val
val.preds.nn$NNpreds = sd(train$kickReturnYardage, na.rm = TRUE) * nn.yard.val.predict + mean(train$kickReturnYardage, na.rm = TRUE)

mae(val$kickReturnYardage, val.preds.nn$NNpreds)
#MAE of 7.074 on val after unscaling predictions


#Variable importance
imp.nn.garson = garson(nn.yard.model)
imp.nn = data.frame(cbind(Feature = levels(imp.nn.garson$data$x_names) ,GarsonImp = imp.nn.garson$data$rel_imp))

imp.nn$scaledImpNN = scale_01(as.numeric(imp.nn$GarsonImp))
```


################################# Offensive XGBoost #################################


```{r}
#Included only listed variables
train_x = model.matrix(kickReturnYardage ~ ., 
                       data = train[,c( 3:5, 62,63,65:100  )])[,-1]
train_y = train$kickReturnYardage

# Build XGBoost model
set.seed(123)
#xgb <- xgb.cv(data = train_x, label = train_y, subsample = 0.5, nrounds = 100, nfold = 10, eval.metric = 'mae')
#test MAE lowest at nrounds = 5

## Caret Tuning
#set seed
set.seed(123)
# Tuning through caret
tune_grid = expand.grid(
    nrounds = 5,
    eta = c(.1, .15, .2, .25, .3),
    max_depth = c(1:10),
    gamma = c(0),
    colsample_bytree = 1,
    min_child_weight = 1,
    subsample = c(.25, .5, .75, 1))

#set seed
set.seed(123)
#build caret tuning
xgb.punt.caret = train(x = train_x, y = train_y, method = "xgbTree", 
                         tuneGrid = tune_grid, 
                         trControl = trainControl(method = 'cv', number = 10),
                         metric = 'MAE')

#plot tuning grid
#plot(xgb.punt.caret)

############################################################################################################
xgb.punt.jamaica <-  xgboost(data = train_x, label = train_y, 
                         subsample = xgb.punt.caret$bestTune$subsample, nrounds = 5, 
                         eta = xgb.punt.caret$bestTune$eta,
                         max_depth = xgb.punt.caret$bestTune$max_depth, 
                         prediction = T, eval_metric = 'mae')
#MAE = 5.199

############################################################################################################

#Add predictions to train.preds data frame
train.off.preds = train
val.off.preds = val

train.off.preds$OFFpreds = predict(xgb.punt.jamaica, type = "response", train_x)
train.preds = merge(train.preds, train.off.preds[c(1,2,105)], by = c("gameId", "playId"))

#Find MAE on validation
#levels(val$return_zone) = c(levels(val$return_zone), "9")
val_x = model.matrix(kickReturnYardage ~ ., data = val[,c(3:5,62,63,65:100  )])[,-1]
val_y = val$kickReturnYardage

val.off.preds$OFFpreds = predict(xgb.punt.jamaica, type = "response", val_x)
MAE(val.off.preds$OFFpreds, val.off.preds$kickReturnYardage)
#Val MAE of 6.062

#Variable Importance
imp.off = xgb.importance(feature_names = colnames(train_x), model = xgb.punt.jamaica)
imp.off$scaledImpOff = scale_01(imp.off$Gain)

```



########################################################################################

############################### Combining the 3 models #################################
```{r}
val.preds = merge(val.preds.xgb, val.preds.nn[c(1,2,105)], by = c("gameId", "playId"))
val.preds = merge(val.preds, val.off.preds[c(1,2,105)], by = c("gameId", "playId"))

#Simple average
mae(train.preds$kickReturnYardage, (train.preds$XGBpreds + train.preds$NNpreds + train.preds$OFFpreds) /3)
#MAE of 4.612

mae(val.preds$kickReturnYardage, (val.preds$XGBpreds + val.preds$NNpreds + val.preds$OFFpreds)/3)
#MAE of 5.967


#Linear model ensemble
lm.ensemble = lm(kickReturnYardage ~ XGBpreds + NNpreds + OFFpreds, data = train.preds)
lm.ensemble
train.preds$ENS_LMpreds = predict(lm.ensemble, train.preds)
val.preds$ENS_LMpreds = predict(lm.ensemble, val.preds)

mae(train.preds$kickReturnYardage, train.preds$ENS_LMpreds)
#MAE of 4.07
mae(val.preds$kickReturnYardage, val.preds$ENS_LMpreds)
#MAE of 6.832


#Neural Network Ensemble
nn.ensemble <- nnet(kickReturnYardage ~ NNpreds + XGBpreds + OFFpreds, data = train.preds, size = 1, linout = TRUE )

train.preds$ENS_NNpreds = predict(nn.ensemble, train.preds)
val.preds$ENS_NNpreds = predict(nn.ensemble, val.preds)

mae(train.preds$kickReturnYardage, train.preds$ENS_NNpreds)
#MAE of 6.607 on train
mae(val.preds$kickReturnYardage, val.preds$ENS_NNpreds)
#MAE of 6.756 on val
```

################## Run final averaged ensemble model on test set #######################
```{r}
###Get test predictions from xgboost on defense
test.preds.h1 = data.frame()
test.preds.h2 = data.frame()

for (i in 1:2){
  if (i == 1) {
      test.h = test %>% filter(return_x < 28)
    }
    else {
      test.h = test %>% filter(return_x >= 28)
    }
  
  test_x = model.matrix(kickReturnYardage ~ ., data = test.h[,c(3:5, 7:39,62,63,65:70 )])[,-1]
  
  test.h$XGBpreds = predict(half_xgbs[i], type = "response", test_x)[[1]]

  #Storing predictions for ensemble model
  assign(paste0("test.preds.h",i), test.h)
}

#Combine predictions into one data set for ensemble
test.preds.xgb = rbind(test.preds.h1, test.preds.h2)


###Get test predictions for neural net
test2 = test

test2 = cbind( test2[1:2], predict(trainScale, test2[3:104]))

test2 <- test2 %>% 
  select(var.sel, kickReturnYardage, gameId, playId)

nn.yard.test.predict <- predict(nn.yard.model, newdata = test2)

#Since predictions are on a scaled target variable, must unscale the predictions
test.preds.nn = test
test.preds.nn$NNpreds = sd(train$kickReturnYardage, na.rm = TRUE) * nn.yard.test.predict + mean(train$kickReturnYardage, na.rm = TRUE)


###Get test predictions for defensive XGBoost
test.off.preds = test

test_x = model.matrix(kickReturnYardage ~ ., data = test[,c(3:5,62,63,65:100  )])[,-1]

test.off.preds$OFFpreds = predict(xgb.punt.jamaica, type = "response", test_x)



test.preds = merge(test.preds.xgb, test.preds.nn[c(1,2,105)], by = c("gameId", "playId"))
test.preds = merge(test.preds, test.off.preds[c(1,2,105)], by = c("gameId", "playId"))

#Simple average
mae(test.preds$kickReturnYardage, (test.preds$XGBpreds + test.preds$NNpreds + test.preds$OFFpreds)/3)
#MAE of 5.663
#6.251 def
#6.328 NN
#5.886 off

train.lm = train[,c(3:39,51:63,65:104)]
lm.naive = lm(kickReturnYardage ~ . ,data = train.lm)
train.lm$lmpreds = predict(lm.naive)
mae(train.lm$kickReturnYardage, train.lm$lmpreds)

val.lm = val[,c(3:39,51:63,65:104)]
val.lm$lmpreds = predict(lm.naive, val.lm)
mae(val.lm$kickReturnYardage, val.lm$lmpreds)

test.lm = test[,c(3:39,51:63,65:104)]
test.lm$lmpreds = predict(lm.naive, test.lm)
mae(test.lm$kickReturnYardage, test.lm$lmpreds)

full.lm = full_set[,c(3:39,51:63,65:104)]
full.lm$lmpreds = predict(lm.naive, full.lm)
mae(full.lm$kickReturnYardage, full.lm$lmpreds)
```




############################ Combined Variable Importance ##############################
```{r}
#Join importance metrics
imp.ensemble = full_join(imp.h[,c(1,5)],imp.off[,c(1,5)] , by = "Feature", all = TRUE)
imp.ensemble = full_join(imp.ensemble, imp.nn[,c(1,3)], by = "Feature", all = TRUE)


#Set null values to 0 to allow addition
imp.ensemble$scaledImpHalf = ifelse(is.na(imp.ensemble$scaledImpHalf), 0, imp.ensemble$scaledImpHalf)
imp.ensemble$scaledImpNN = ifelse(is.na(imp.ensemble$scaledImpNN), 0, imp.ensemble$scaledImpNN)
imp.ensemble$scaledImpOff = ifelse(is.na(imp.ensemble$scaledImpOff), 0, imp.ensemble$scaledImpOff)

#Find average of importance metrics and sort in descending order
imp.ensemble$avgScaledImp = (imp.ensemble$scaledImpHalf + imp.ensemble$scaledImpNN + imp.ensemble$scaledImpOff)/3
imp.ensemble = imp.ensemble %>% arrange(desc(avgScaledImp))


```


#################### Applying model to determine best returner/teams ###################


```{r}
#Calculate Defense XGB predictions on full data set
full.h1 = full_set %>% filter(return_x < 28)
full.h2 = full_set %>% filter(return_x >= 28)


half_xgbs = list(xgb.punt.h1, xgb.punt.h2)

full.preds = data.frame()
for (i in 1:2){
  if (i == 1) {
      full.h = full_set %>% filter(return_x < 28)
    }
    else {
      full.h = full_set %>% filter(return_x >= 28)
    }
  
  full_x = model.matrix(kickReturnYardage ~ ., data = full.h[,c(3:5, 7:39,62,63,65:70 )])[,-1]
  full_y = full.h$kickReturnYardage
  
  full.h$XGBpreds = predict(half_xgbs[i], type = "response", full_x)[[1]]
  full.preds = rbind(full.h, full.preds)

}



#Calculate NN predictions on full data set

full2 = cbind( full_set[1:2], predict(trainScale, full_set[3:104]))

full2 <- full2 %>% 
  select(var.sel,  kickReturnYardage, gameId, playId)

nn.yard.full.predict <- predict(nn.yard.model, newdata = full2)
full2$NNpreds = sd(train$kickReturnYardage, na.rm = TRUE) * nn.yard.full.predict + mean(train$kickReturnYardage, na.rm = TRUE)


#Calculate Offense XGB predictions on full data set

full3 = full_set

full_x = model.matrix(kickReturnYardage ~ ., data = full3[,c( 3:5, 62,63,65:100 )])[,-1]
full_y = full3$kickReturnYardage
  
full3$OFFpreds = predict(xgb.punt.jamaica, type = "response", full_x)[[1]]




#Combine both full sets
full.preds = merge(full.preds, full2[37:39], by = c("gameId", "playId"))
full.preds = merge(full.preds, full3[c(1,2,134)], by = c("gameId", "playId"))

```



```{r}
full.preds$ENS_AVGpreds = (full.preds$NNpreds + full.preds$XGBpreds + full.preds$OFFpreds)/3

full.preds$yard_dif = full.preds$kickReturnYardage - full.preds$ENS_AVGpreds

#Find best & worst returners
full.preds %>% group_by(returnerId, punt_returner_player_name) %>% summarise(yards_over_expected = mean(yard_dif)) %>% arrange(desc(yards_over_expected))

#Find best & worst teams with getting in position
full.preds %>% group_by(possessionTeam) %>% summarise(yards_expected = mean(ENS_AVGpreds)) %>% arrange(desc(yards_expected))

#Find best & worst teams with punt coverage compared to expected
full.preds %>% group_by(possessionTeam) %>% summarise(yards_over_expected = mean(yard_dif)) %>% arrange(desc(yards_over_expected))


```
