)
set.seed(123)
xgb.punt.caret = train(x = train_x, y = train_y, method = "xgbTree", tuneGrid = tune_grid, trControl = trainControl(method = 'cv', number = 10), metric = 'MAE')
set.seed(123)
assign(paste0("xgb.punt.h", i), xgboost(data = train_x, label = train_y,
subsample = xgb.punt.caret$bestTune$subsample, nrounds = 4,
eta = xgb.punt.caret$bestTune$eta,
max_depth = xgb.punt.caret$bestTune$max_depth,
prediction = T, eval_metric = 'mae'))
set.seed(123)
xgb.punt.h = xgboost(data = train_x, label = train_y,
subsample = xgb.punt.caret$bestTune$subsample, nrounds = 4,
eta = xgb.punt.caret$bestTune$eta,
max_depth = xgb.punt.caret$bestTune$max_depth,
prediction = T, eval_metric = 'mae')
#x = xgb.importance(feature_names = colnames(train_x), model = xgb.punt.final)
#print(x)
g = xgb.ggplot.importance(xgb.importance(feature_names = colnames(train_x), model = xgb.punt.h)) + ggtitle(paste("Feature Importance Half", i))
print(g)
}
averaged_mae = (min(xgb.punt.h1$evaluation_log$train_mae)*nrow(train.h1) +
min(xgb.punt.h2$evaluation_log$train_mae)*nrow(train.h2))/nrow(train)
print(num)
print(vars)
print(averaged_mae)
}
train2 <- train[,c(4:40,63:68,70,71 )]
View(train2)
train3 <- train[,c(4:6, 8:40,63:71 )]
library(tidyverse)
library(caret)
library(leaps)
library(glmnet)
library(ggplot2)
library(earth)
library(mgcv)
library(ROCR)
library(Cairo)
library(pROC)
library(ROCR)
library(randomForest)
library(xgboost)
library(Ckmeans.1d.dp)
library(pdp)
library(Matrix)
library(gganimate)
library(cowplot)
library(ggridges)
library(repr)
library(gifski)
library(plotly)
library(rpart)
library(rpart.plot)
library(nnet)
library(NeuralNetTools)
library(Metrics)
#turning off warnings
options(warn=-1)
# Set directory
setwd("C:\\Users\\liamd\\Documents\\GitHub\\NFL_Big_Data_Bowl_2022\\data_folder")
# Read in data
train <- read.csv("model train set.csv")
val <- read.csv("model val set.csv")
test <- read.csv("model test set.csv")
train3 <- train[,c(4:40,63:68,70,71 )]
val3 <- val[,c(4:40,63:68,70,71 )]
test3 <- test[,c(4:40,63:68,70,71 )]
View(train3)
# Any variables with less than 10 values are labeled at categorical (13 because I wanted zone variables to be a factor)
for (i in 1:length(train3)) {
if (length(unique(train3[,i])) < 16) {
train3[,i] = as.factor(train3[,i])
}
}
# Filter out NA observations
train3 = train3 %>% filter(is.na(train3$kickReturnYardage) == 0)
# Scale standardize
for (i in 4:length(train3)) {
if (class(train3[,i]) == c('numeric', 'integer')) {
train3[,i] = scale(train3[,i])
}
}
# Flag Outliers
outliers <- boxplot(train3$kickReturnYardage, plot=FALSE)$out
train3['outlier_flag'] <- ifelse(train3$kickReturnYardage %in% outliers, "1", "0")
train3$outlier_flag <- as.factor(train3$outlier_flag)
# Any variables with less than 10 values are labeled at categorical (13 because I wanted zone variables to be a factor)
for (i in 1:length(val3)) {
if (length(unique(val3[,i])) < 16) {
val3[,i] = as.factor(val3[,i])
}
}
# Filter out NA observations
val3 = val3 %>% filter(is.na(val3$kickReturnYardage) == 0)
# Scale standardize
for (i in 4:length(val3)) {
if (class(val3[,i]) == c('numeric', 'integer')) {
val3[,i] = scale(val3[,i])
}
}
# Flag outliers
outliersval <- boxplot(val3$kickReturnYardage, plot=FALSE)$out
val3['outlier_flag'] <- ifelse(val3$kickReturnYardage %in% outliersval, "1", "0")
val3$outlier_flag <- as.factor(val3$outlier_flag)
# Any variables with less than 10 values are labeled at categorical (13 because I wanted zone variables to be a factor)
for (i in 1:length(test3)) {
if (length(unique(test3[,i])) < 16) {
test3[,i] = as.factor(test3[,i])
}
}
# Filter out NA observations
test3 = test3 %>% filter(is.na(test3$kickReturnYardage) == 0)
# Scale standardize
for (i in 4:length(test3)) {
if (class(test3[,i]) == c('numeric', 'integer')) {
test3[,i] = scale(test3[,i])
}
}
# Flag outliers
outlierstest <- boxplot(test3$kickReturnYardage, plot=FALSE)$out
test3['outlier_flag'] <- ifelse(test3$kickReturnYardage %in% outlierstest, "1", "0")
test3$outlier_flag <- as.factor(test3$outlier_flag)
set.seed(123)
nn.yard.model <- nnet(kickReturnYardage ~ . -gameId -playId, data = train3, size = 6, decay = .945, linout = TRUE )
set.seed(123)
nn.yard.model <- nnet(kickReturnYardage ~ . , data = train3, size = 6, decay = .945, linout = TRUE )
set.seed(123)
nn.yard.model3 <- nnet(kickReturnYardage ~ . , data = train3, size = 6, decay = .945, linout = TRUE )
nn.yard.val3.predict <- predict(nn.yard.model3, newdata = val3)
# Calculate RMSE and MAE
val3.rmse <- sqrt(mean((nn.yard.val3.predict - val3$kickReturnYardage)^2)) # RMSE
val3.mae <- mae(val3$kickReturnYardage, nn.yard.val3.predict) # MAE
nn.yard.test3.predict <- predict(nn.yard.model3, newdata = test3)
# Calculate RMSE and MAE
test3.rmse <- sqrt(mean((nn.yard.test3.predict - test3$kickReturnYardage)^2)) # RMSE
test3.mae <- mae(test3$kickReturnYardage, nn.yard.test3.predict) # MAE
library(tidyverse)
library(caret)
library(leaps)
library(glmnet)
library(ggplot2)
library(earth)
library(mgcv)
library(ROCR)
library(Cairo)
library(pROC)
library(ROCR)
library(randomForest)
library(xgboost)
library(Ckmeans.1d.dp)
library(pdp)
library(Matrix)
library(gganimate)
library(cowplot)
library(ggridges)
library(repr)
library(gifski)
library(plotly)
library(rpart)
library(rpart.plot)
library(nnet)
library(NeuralNetTools)
library(Metrics)
#turning off warnings
options(warn=-1)
# Set directory
setwd("C:\\Users\\liamd\\Documents\\GitHub\\NFL_Big_Data_Bowl_2022\\data_folder")
# Read in data
train <- read.csv("model train set.csv")
val <- read.csv("model val set.csv")
test <- read.csv("model test set.csv")
train3 <- train[,c(4:40,63:68,70,71 )]
val3 <- val[,c(4:40,63:68,70,71 )]
test3 <- test[,c(4:40,63:68,70,71 )]
View(test)
library(tidyverse)
library(caret)
library(leaps)
library(glmnet)
library(ggplot2)
library(earth)
library(mgcv)
library(ROCR)
library(Cairo)
library(pROC)
library(ROCR)
library(randomForest)
library(xgboost)
library(Ckmeans.1d.dp)
library(pdp)
library(Matrix)
library(gganimate)
library(cowplot)
library(ggridges)
library(repr)
library(gifski)
library(plotly)
library(rpart)
library(rpart.plot)
library(nnet)
library(NeuralNetTools)
library(Metrics)
#turning off warnings
options(warn=-1)
# Set directory
setwd("C:\\Users\\liamd\\Documents\\GitHub\\NFL_Big_Data_Bowl_2022\\data_folder")
# Read in data
train <- read.csv("model train set.csv")
val <- read.csv("model val set.csv")
test <- read.csv("model test set.csv")
train3 <- train[,c(4:40,63:64,66:68,70,71 )]
val3 <- val[,c(4:40,63:64,66:68,70,71 )]
test3 <- test[,c(4:40,63:64,66:68,70,71 )]
# Remove def zone variables & actual return direction
train <- train %>%
select(-41:-51, -65)
# Any variables with less than 10 values are labeled at categorical (13 because I wanted zone variables to be a factor)
for (i in 1:length(train)) {
if (length(unique(train[,i])) < 16) {
train[,i] = as.factor(train[,i])
}
}
# Filter out NA observations
train = train %>% filter(is.na(train$kickReturnYardage) == 0)
#Getting variable importance from decision tree
fit = rpart(train$kickReturnYardage ~ . , data = train[,c(4:60)])
library(tidyverse)
library(caret)
library(leaps)
library(glmnet)
library(ggplot2)
library(earth)
library(mgcv)
library(ROCR)
library(Cairo)
library(pROC)
library(ROCR)
library(randomForest)
library(xgboost)
library(Ckmeans.1d.dp)
library(pdp)
library(Matrix)
library(gganimate)
library(cowplot)
library(ggridges)
library(repr)
library(gifski)
library(plotly)
library(rpart)
library(rpart.plot)
library(nnet)
library(NeuralNetTools)
library(Metrics)
#turning off warnings
options(warn=-1)
# Set directory
setwd("C:\\Users\\liamd\\Documents\\GitHub\\NFL_Big_Data_Bowl_2022\\data_folder")
# Read in data
train <- read.csv("model train set.csv")
val <- read.csv("model val set.csv")
test <- read.csv("model test set.csv")
train3 <- train[,c(4:40,63:64,66:68,70,71 )]
val3 <- val[,c(4:40,63:64,66:68,70,71 )]
test3 <- test[,c(4:40,63:64,66:68,70,71 )]
# Remove def zone variables & actual return direction
train <- train %>%
select(-41:-51, -65)
# Any variables with less than 10 values are labeled at categorical (13 because I wanted zone variables to be a factor)
for (i in 1:length(train)) {
if (length(unique(train[,i])) < 16) {
train[,i] = as.factor(train[,i])
}
}
# Filter out NA observations
train = train %>% filter(is.na(train$kickReturnYardage) == 0)
#Getting variable importance from decision tree
fit = rpart(train$kickReturnYardage ~ . , data = train[,c(4:59)])
rpart.plot(fit)
varimp.data = data.frame(fit$variable.importance)
varimp.data$names = as.character(rownames(varimp.data))
varimp.data
# Check columns data types
sapply(train, class) # Dataset contains numeric, integer, and factor variables
# Scale standardize
for (i in 4:length(train)) {
if (class(train[,i]) == c('numeric', 'integer')) {
train[,i] = scale(train[,i])
}
}
# Using varimp.data output. Cutoff of 700 for fit.variable.importance
var.sel <- varimp.data$names
train2 <- train %>%
select(var.sel, kickReturnYardage, X, gameId, playId)
# Remove def zone variables & actual return direction
val <- val %>%
select(-41:-51, -65)
# Any variables with less than 10 values are labeled at categorical (16 because I wanted zone variables to be a factor)
for (i in 1:length(val)) {
if (length(unique(val[,i])) < 16) {
val[,i] = as.factor(val[,i])
}
}
# Filter out NA observations
val <-  val %>%
filter(is.na(val$kickReturnYardage) == 0)
# Scale standardize
for (i in 4:length(val)) {
if (class(val[,i]) == c('numeric', 'integer')) {
val[,i] = scale(val[,i])
}
}
val$kickReturnYardage <- as.numeric(val$kickReturnYardage)
# Subset variables
val2 <- val %>%
select(var.sel, X, kickReturnYardage, gameId, playId)
# Remove def zone variables & actual return direction
test <- test %>%
select(-41:-51, -65)
# Any variables with less than 10 values are labeled at categorical (16 because I wanted zone variables to be a factor)
for (i in 1:length(test)) {
if (length(unique(test[,i])) < 16) {
test[,i] = as.factor(test[,i])
}
}
# Filter out NA observations
test <-  test %>%
filter(is.na(test$kickReturnYardage) == 0)
# Scale standardize
for (i in 4:length(test)) {
if (class(test[,i]) == c('numeric', 'integer')) {
test[,i] = scale(test[,i])
}
}
test$kickReturnYardage <- as.numeric(test$kickReturnYardage)
# Subset variables
test2 <- test %>%
select(var.sel, X, kickReturnYardage, gameId, playId)
set.seed(123)
nn.yard.model <- nnet(kickReturnYardage ~ . -gameId -playId, data = train2, size = 6, decay = .945, linout = TRUE )
nn.yard.val.predict <- predict(nn.yard.model, newdata = val2)
# Calculate RMSE and MAE
val.rmse <- sqrt(mean((nn.yard.val.predict - val2$kickReturnYardage)^2)) # RMSE
val.mae <- mae(val2$kickReturnYardage, nn.yard.val.predict) # MAE
nn.yard.test.predict <- predict(nn.yard.model, newdata = test2)
# Calculate RMSE and MAE
test.rmse <- sqrt(mean((nn.yard.test.predict - test2$kickReturnYardage)^2)) # RMSE
test.mae <- mae(test2$kickReturnYardage, nn.yard.test.predict) # MAE
set.seed(123)
nn.yard.model <- nnet(kickReturnYardage ~ . -gameId -playId, data = train, size = 6, decay = .945, linout = TRUE )
nn.yard.val.predict <- predict(nn.yard.model, newdata = val)
# Calculate RMSE and MAE
val.rmse <- sqrt(mean((nn.yard.val.predict - val$kickReturnYardage)^2)) # RMSE
val.mae <- mae(val2$kickReturnYardage, nn.yard.val.predict) # MAE
nn.yard.test.predict <- predict(nn.yard.model, newdata = test)
# Calculate RMSE and MAE
test.rmse <- sqrt(mean((nn.yard.test.predict - test2$kickReturnYardage)^2)) # RMSE
test.mae <- mae(test$kickReturnYardage, nn.yard.test.predict) # MAE
# Any variables with less than 10 values are labeled at categorical (13 because I wanted zone variables to be a factor)
for (i in 1:length(train3)) {
if (length(unique(train3[,i])) < 16) {
train3[,i] = as.factor(train3[,i])
}
}
# Filter out NA observations
train3 = train3 %>% filter(is.na(train3$kickReturnYardage) == 0)
# Scale standardize
for (i in 4:length(train3)) {
if (class(train3[,i]) == c('numeric', 'integer')) {
train3[,i] = scale(train3[,i])
}
}
# Any variables with less than 10 values are labeled at categorical (13 because I wanted zone variables to be a factor)
for (i in 1:length(val3)) {
if (length(unique(val3[,i])) < 16) {
val3[,i] = as.factor(val3[,i])
}
}
# Filter out NA observations
val3 = val3 %>% filter(is.na(val3$kickReturnYardage) == 0)
# Scale standardize
for (i in 4:length(val3)) {
if (class(val3[,i]) == c('numeric', 'integer')) {
val3[,i] = scale(val3[,i])
}
}
# Any variables with less than 10 values are labeled at categorical (13 because I wanted zone variables to be a factor)
for (i in 1:length(test3)) {
if (length(unique(test3[,i])) < 16) {
test3[,i] = as.factor(test3[,i])
}
}
# Filter out NA observations
test3 = test3 %>% filter(is.na(test3$kickReturnYardage) == 0)
# Scale standardize
for (i in 4:length(test3)) {
if (class(test3[,i]) == c('numeric', 'integer')) {
test3[,i] = scale(test3[,i])
}
}
set.seed(123)
nn.yard.model3 <- nnet(kickReturnYardage ~ . , data = train3, size = 6, decay = .945, linout = TRUE )
nn.yard.val3.predict <- predict(nn.yard.model3, newdata = val3)
# Calculate RMSE and MAE
val3.rmse <- sqrt(mean((nn.yard.val3.predict - val3$kickReturnYardage)^2)) # RMSE
val3.mae <- mae(val3$kickReturnYardage, nn.yard.val3.predict) # MAE
nn.yard.test3.predict <- predict(nn.yard.model3, newdata = test3)
# Calculate RMSE and MAE
test3.rmse <- sqrt(mean((nn.yard.test3.predict - test3$kickReturnYardage)^2)) # RMSE
test3.mae <- mae(test3$kickReturnYardage, nn.yard.test3.predict) # MAE
View(train3)
# Model Tuning
tune_grid <- expand.grid(
.size = c(4, 5, 6, 7, 8),
.decay = c(0, .25, .5, .75, 1)
)
set.seed(123)
nn.yards.caret <- train(kickReturnYardage ~ .,
data = train3,
method = "nnet",
#metric = "RMSE",
tuneGrid = tune_grid,
trControl = trainControl(method = 'cv', number = 10),
trace = FALSE,
linout = TRUE,
MaxNWts = 3000)
tune.results <- as.data.frame(nn.yards.caret$results)
# Get parameters for best RMSE and for best MAE
tune.results[which.min(tune.results$MAE),]
# Model Tuning
tune_grid <- expand.grid(
.size = c(4, 5, 6, 7, 8),
.decay = seq(.15, .35, by = .02)
)
set.seed(123)
nn.yards.caret <- train(kickReturnYardage ~ .,
data = train3,
method = "nnet",
#metric = "RMSE",
tuneGrid = tune_grid,
trControl = trainControl(method = 'cv', number = 10),
trace = FALSE,
linout = TRUE,
MaxNWts = 3000)
tune.results <- as.data.frame(nn.yards.caret$results)
# Get parameters for best RMSE and for best MAE
tune.results[which.min(tune.results$MAE),]
# Model Tuning
tune_grid <- expand.grid(
.size = c(2, 3, 4, 5),
.decay = seq(.3, .4, by = .01)
)
set.seed(123)
nn.yards.caret <- train(kickReturnYardage ~ .,
data = train3,
method = "nnet",
#metric = "RMSE",
tuneGrid = tune_grid,
trControl = trainControl(method = 'cv', number = 10),
trace = FALSE,
linout = TRUE,
MaxNWts = 3000)
tune.results <- as.data.frame(nn.yards.caret$results)
# Get parameters for best RMSE and for best MAE
tune.results[which.min(tune.results$MAE),]
set.seed(123)
# Model Tuning
tune_grid <- expand.grid(
.size = c(2, 3, 4, 5, 6, 7),
.decay = seq(.3, .4, by = .01)
)
set.seed(123)
nn.yards.caret <- train(kickReturnYardage ~ .,
data = train3,
method = "nnet",
#metric = "RMSE",
tuneGrid = tune_grid,
trControl = trainControl(method = 'cv', number = 10),
trace = FALSE,
linout = TRUE,
MaxNWts = 3000)
tune.results <- as.data.frame(nn.yards.caret$results)
# Get parameters for best RMSE and for best MAE
tune.results[which.min(tune.results$MAE),]
set.seed(123)
nn.yard.model3 <- nnet(kickReturnYardage ~ . , data = train3, size = 3, decay = .35, linout = TRUE )
nn.yard.val3.predict <- predict(nn.yard.model3, newdata = val3)
# Calculate RMSE and MAE
val3.rmse <- sqrt(mean((nn.yard.val3.predict - val3$kickReturnYardage)^2)) # RMSE
val3.mae <- mae(val3$kickReturnYardage, nn.yard.val3.predict) # MAE
nn.yard.test3.predict <- predict(nn.yard.model3, newdata = test3)
# Calculate RMSE and MAE
test3.rmse <- sqrt(mean((nn.yard.test3.predict - test3$kickReturnYardage)^2)) # RMSE
test3.mae <- mae(test3$kickReturnYardage, nn.yard.test3.predict) # MAE
set.seed(123)
# Model Tuning
tune_grid <- expand.grid(
.size = c(2, 3, 4, 5, 6, 7),
.decay = c(0, .25, .5, .75, 1)
)
set.seed(123)
nn.yards.caret <- train(kickReturnYardage ~ .,
data = train2,
method = "nnet",
#metric = "RMSE",
tuneGrid = tune_grid,
trControl = trainControl(method = 'cv', number = 10),
trace = FALSE,
linout = TRUE,
MaxNWts = 3000)
tune.results <- as.data.frame(nn.yards.caret$results)
# Get parameters for best RMSE and for best MAE
tune.results[which.min(tune.results$MAE),]
